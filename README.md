# Context-Aware QA System

This project implements a Context-Aware Question Answering system designed to operate under strict resource constraints: a **Local LLM with fewer than 2 billion parameters** and a **maximum context window of 1024 tokens**.

It was developed as part of the "Context-Aware QA" technical challenge.

## Project Context & Objective

The goal of this project is to design a question-answering system that intelligently selects and presents the most relevant information to a small local language model. Due to the limited context window (1024 tokens), the system cannot simply feed all available documents to the model. Instead, it must employ efficient retrieval and ranking strategies to ensure high-quality answers without exceeding the token budget.

## Key Features

* **Intelligent Reranking:** Uses a Cross-Encoder to re-score and sort retrieved documents, ensuring the most relevant chunks are prioritized.
* **Strict Token Management:** Dynamically calculates token usage to fill the context window up to the limit without truncation, maximizing information density.
* **Source Grounding:** Injects metadata (filenames and headers) directly into text chunks to help the model understand the origin of information.
* **Local Inference:** Runs entirely on local hardware using `llama.cpp` and quantized GGUF models (e.g., Qwen, Gemma, Llama, ...).
* **Automated Evaluation:** Includes a "LLM-as-a-Judge" pipeline using Gemini to evaluate answer quality (Correctness, Completeness, Hallucination) against a ground truth.

## Key Documentation

The following documents provide in-depth analysis of the project's methodology and results:

* **[ARCHITECTURE](ARCHITECTURE.md)**: Detailed analysis of technical choices, architectural decisions, and trade-offs (Chunking, Embeddings, Models). **TODO**
* **[EVALUATION](EVALUATION.md)**: Critical review of the system's performance, including success cases, limitations, and future improvements. **TODO**

## Architecture

The pipeline follows a robust RAG (Retrieval-Augmented Generation) approach:

1. **Ingestion:** Documents are cleaned, split by Markdown headers, and chunked. Metadata is injected into each chunk for context.
2. **Retrieval:** `sentence-transformers/all-mpnet-base-v2` identifies conceptually related text.
3. **Reranking:** A Cross-Encoder refines the selection, drastically improving precision.
4. **Context Construction:** The top-ranked chunks are selected until the 1024-token limit is reached.
5. **Generation:** A quantized local model (default: `Qwen2.5-1.5B-Instruct`) generates the answer using a strict system prompt to minimize hallucinations.

## Installation

### Prerequisites

* Python 3.10 or higher
* (Optional) A Google Gemini API key for running the evaluation script.

### Setup

1. **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

2. **Environment Configuration:**
    Set the `GEMINI_API_KEY` environment variable if you plan to run the evaluation script (support for `.env` files)

    ```bash
    GEMINI_API_KEY=your_api_key_here
    ```

### Model Downloads

The system automatically checks for and downloads the required GGUF models from HuggingFace upon the first run. No manual download is required.

## Usage

The project provides a single entry point `src/main.py` for all main operations.

### Interactive Chat Mode

To start a conversation with the technical assistant using the local model:

```bash
python src/main.py --mode chat
```

*Type 'exit' to quit.*

### Batch Generation Mode

To process the predefined questions in `data/questions.json` and generate answers:

```bash
python src/main.py --mode batch --output data/results.json
```

### Evaluation

To evaluate the generated answers against the ground truth using the "LLM-as-a-Judge" method:

```bash
python src/evaluate.py --results data/results.json
```

This script uses **Gemini 2.5 Flash** to audit the RAG pipeline. It compares the generated answers against the reference provided in `data/ground_truth.json` (which was generated by **Gemini 3.0 Pro**).

> **Note:** You can specify a different ground truth file using the `--ground-truth` argument (e.g., `--ground-truth data/custom_ground_truth.json`).

This will output scores for Correctness, Recall, and Precision, and detect potential hallucinations.

### Advanced Options

The CLI supports several arguments to customize the pipeline's behavior:

#### 1. Choose a Model (`--model`)

Select the local LLM used for answer generation.

```bash
python src/main.py --mode chat --model phi
```

**Available Models:**

* `qwen` (Default): **Qwen2.5-1.5B-Instruct**. Best balance of speed and instruction following.
* `gemma`: **Gemma-2-2b-it**. Google's official lightweight model.
* `llama`: **Llama-3.2-1B-Instruct**. Meta's official lightweight model.
* `phi`: **Phi-3-mini-4k**. High quality, larger context understanding (but too many parameters).
* `exaone`: **EXAONE-3.5-2.4B**. High performance on instruction following benchmarks.
* `granite`: **Granite-3.1-2b**. Solid all-rounder.
* `benchmaxx`: **Llama-3.2-1B-Instruct** (Benchmaxx). Optimized for reasoning tasks.

#### 2. Choose a Reranker (`--reranker`)

Select the Cross-Encoder model used to score and sort retrieved documents.

```bash
python src/main.py --mode chat --reranker ms-marco
```

**Available Rerankers:**

* `bge` (Default): **BAAI/bge-reranker-v2-m3**. Modern, high-performance reranker.
* `ms-marco`: **cross-encoder/ms-marco-MiniLM-L-6-v2**. Older but very reliable for strict keyword matching and relevance.

#### 3. Verbose Mode (`-v` or `--verbose`)

Enable detailed logging to see exactly what the system is doing (retrieved chunks, scores, token usage).

```bash
python src/main.py --mode chat -v
```

#### 4. Custom Output (`--output`)

In batch mode, specify where to save the JSON results.

```bash
python src/main.py --mode batch --output data/my_experiment_results.json
```

## Project Structure

* `data/`: Contains source documents (`docs/`), questions, and evaluation datasets.
* `src/`: Source code.
  * `main.py`: CLI entry point.
  * `rag.py`: Core RAG pipeline implementation.
  * `ingestion.py`: vector database creation and indexing.
  * `evaluate.py`: Evaluation script.
  * `config.py`: Central configuration for paths and model parameters.
* `models/`: Directory where GGUF models are downloaded.
